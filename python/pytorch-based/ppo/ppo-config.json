{
    "Number of runs": 1, 
    
    "Value discount (gamma)": 0.99, 
    "Number of training episodes": 1000, 
    "Maximum steps in training episode (for train environment termination)": 100000, 
    "Maximum steps in evaluation episode (for evaluation environment termination)": 100,
    "Number of evaluation episodes": 10,
    
    "Episode interval for evaluation": 10, 
    "Number of steps in a collected trajectory": 100,
    "Number of trajectories used for training per episode": 30,
    "Number of steps in a minibatch": 100,

    "Maximum unique NFE": 30000000,
    
    "Compute periodic returns": true,
    "Continuous minibatch": false,
    "Sample minibatch": false,
    "Buffer used": false,
    "Replay buffer capacity": 100000,
    "Normalize advantages": true,
    "Discrete actions": true,
    "Advantage discount (lambda)": 0.9,
    "Use new problem formulation": true,
    "Include weights in state": false,
    
    "Actor network layer units": [200, 100, 200],
    "Actor network dropout probabilities": [0.20, 0.10, 0.20],
    "Critic network layer units": [200, 100, 60],
    "Critic network dropout probabilities": [0.20, 0.10, 0.05],
    
    "Clipping ratio threshold": 0.1,
    "Use entropy loss bonus": true,
    "Entropy coefficient": 0.5,

    "Clip gradient norm": false,
    "Max gradient norm": 1.0,
    
    "Number of training epochs": 30,
    
    "Initial training learning rate": 1e-4,
    "Learning rate decay rate": 0.95,
    "Learning rate decay steps": 1000,
    "RMSprop optimizer velocity discount rate": 0.9,
    "RMSprop optimizer momentum": 0.1,

    "Episode interval to save actor and critic networks": 5,
    
    "Savepath": "C:\\SEAK Lab\\SEAK Lab Github\\Heuristics in RL\\python\\pytorch-based\\results\\"
}