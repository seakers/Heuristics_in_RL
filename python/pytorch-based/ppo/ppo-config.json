{
    "Number of runs": 1, 
    
    "Value discount (gamma)": 0.99, 
    "Number of training episodes": 50, 
    "Maximum steps in evaluation episode": 60,

    "Episode interval for evaluation": 10, 
    "Number of steps in a collected trajectory": 60,
    "Number of trajectories used for training per episode": 50,
    "Number of steps in a minibatch": 60,

    "Maximum unique NFE": 5000000,
    
    "Compute periodic returns": true,
    "Continuous minibatch": false,
    "Sample minibatch": false,
    "Buffer used": false,
    "Replay buffer capacity": 100000,
    "Normalize advantages": false,
    "Discrete actions": true,
    "Advantage discount (lambda)": 0.9,
    "Use new problem formulation": true,
    "Include weights in state": true,
    
    "Actor network layer units": [200, 100, 200],
    "Actor network dropout probabilities": [0.20, 0.10, 0.20],
    "Critic network layer units": [200, 100, 60],
    "Critic network dropout probabilities": [0.20, 0.10, 0.05],
    
    "Clipping ratio threshold": 0.1,
    "Use entropy loss bonus": true,
    "Entropy coefficient": 1000,

    "Clip gradient norm": true,
    "Max gradient norm": 1.0,
    "Critic loss coefficient": 0.0001,
    
    "Number of training epochs": 30,
    
    "Initial training learning rate": 1e-4,
    "Learning rate decay rate": 0.95,
    "Learning rate decay steps": 5000,
    "RMSprop optimizer velocity discount rate": 0.9,
    "RMSprop optimizer momentum": 0.1,

    "Episode interval to save actor and critic networks": 5,
    
    "Savepath": "C:\\SEAK Lab\\SEAK Lab Github\\Heuristics in RL\\python\\pytorch-based\\results\\"
}