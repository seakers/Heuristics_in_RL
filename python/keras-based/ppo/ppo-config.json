{
    "Number of runs": 5, 
    
    "Value discount (gamma)": 0.99, 
    "Number of training episodes": 500, 
    "Maximum steps in training episode (for train environment termination)": 1000000, 
    "Maximum steps in evaluation episode (for evaluation environment termination)": 100,
    "Number of evaluation episodes": 10,
    
    "Episode interval for evaluation": 10, 
    "Initial number of stored trajectories": 100,
    "Number of steps in a collected trajectory": 100,
    "Number of trajectories used for training per episode": 30,
    "Number of steps in a minibatch": 75,

    "Maximum unique NFE": 6000,
    
    "Compute periodic returns": true,
    "Continuous minibatch": false,
    "Buffer used": false,
    "Replay buffer capacity": 100000,
    "Normalize advantages": true,
    "Discrete actions": true,
    "Advantage discount (lambda)": 0.9,
    "Use new problem formulation": true,
    "Include weights in state": false,
    
    "Actor network layer units": [200, 200],
    "Actor network dropout probabilities": [0.10, 0.10],
    "Critic network layer units": [200, 200],
    "Critic network dropout probabilities": [0.10, 0.10],
    
    "Use clipping loss": true,
    "Clipping ratio threshold": 0.1,
    "Use Adaptive KL penalty loss": false,
    "KL target": 0.005,
    "Adaptive KL coefficient (beta)": 1,
    "Use entropy loss bonus": true,
    "Entropy coefficient": 0.5,
    "Use early stopping for actor training": true,
    
    "Number of actor training iterations": 30,
    "Number of critic training iterations": 60,
    
    "Initial actor training learning rate": 1e-4,
    "Initial critic training learning_rate": 2e-4,
    "Learning rate decay rate": 0.95,
    "Learning rate decay steps (actor)": 1000,
    "Learning rate decay steps (critic)": 1000,
    "RMSprop optimizer rho": 0.9,
    "RMSprop optimizer momentum": 0.1,

    "Episode interval to save actor and critic networks": 5,
    
    "Savepath": "C:\\SEAK Lab\\SEAK Lab Github\\Heuristics in RL\\python\\keras-based\\results\\ppo\\"
}